<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MARINA Plus - Technical Algorithm Report</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>
    <link rel="stylesheet" href="styles.css">

</head>
<body>
    <div class="bg-animation"></div>

    <header>
        <div class="header-content">
            <div class="logo-area">
                <img src="MarinaLogo.png" alt="MARINA Plus Logo" class="logo-img">
            </div>

            <nav>
                <ul>
                    <li><a href="#overview" class="active"><span>Overview</span></a></li>
                    <li><a href="#yolov8"><span>YOLOv8</span></a></li>
                    <li><a href="#comparison"><span>Model Comparison</span></a></li>
                    <li><a href="#hardware"><span>Hardware</span></a></li>
                </ul>
            </nav>
        </div>
    </header>

    <div class="hero">
        <h2>CNN Algorithm Report</h2>
        <p class="hero-subtitle">YOLOv8 Implementation for Tuna Fish Detection & Biomass Estimation with Raspberry Pi Integration</p>
        
        <div class="stats-grid">
            <div class="stat-card">
                <div class="stat-number">YOLOv8</div>
                <div class="stat-label">Detection Model</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">5-Stage</div>
                <div class="stat-label">Pipeline Architecture</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">RPi 4</div>
                <div class="stat-label">Edge Device</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">35+ FPS</div>
                <div class="stat-label">Target Performance</div>
            </div>
        </div>
    </div>

    <div class="container">

        <!-- AI System Pipeline -->
        <section id="ai-pipeline" class="fade-in">
            <div class="section-header">
                <h2 class="section-title">Complete AI System Pipeline</h2>
                <p class="section-subtitle">Multi-stage intelligent processing for comprehensive fish monitoring</p>
            </div>

            <div class="glass-card" style="margin-bottom: 3rem;">
                <h3 style="font-size: 1.8rem; margin-bottom: 2rem; color: white;">5-Stage Processing Architecture</h3>
                <p style="color: var(--text-muted); font-size: 1.1rem; line-height: 1.8; margin-bottom: 2rem;">
                    The MARINA Plus system employs a sophisticated multi-branch pipeline that processes camera input through five specialized stages. After initial detection, the system intelligently routes fish to different processing branches based on their view angle, ensuring optimal accuracy for each scenario.
                </p>

                <div class="pipeline" style="grid-template-columns: 1fr; gap: 2rem; margin-top: 3rem;">
                    <div class="pipeline-step" style="background: linear-gradient(135deg, rgba(102, 126, 234, 0.15), rgba(240, 147, 251, 0.15));">
                        <div class="step-number">1</div>
                        <h3 class="step-title">YOLOv8n Detection Stage</h3>
                        <div class="step-model">Primary Detection & Segmentation</div>
                        <div class="step-specs" style="background: rgba(102, 126, 234, 0.2);">
                            <strong>Stage 1: Complete & Optimized</strong><br>
                            Input: Stereo camera feed (640×640)<br>
                            Output: Fish bounding boxes + segmentation masks<br>
                            Performance: 20-25 FPS (FP32), 35-40 FPS (INT8)
                        </div>
                        <p style="color: var(--text-muted); margin-top: 1rem; line-height: 1.7;">
                            YOLOv8-Nano serves as the entry point, detecting all fish in the frame and generating precise segmentation masks. This stage identifies fish presence and location, feeding data to subsequent specialized modules. <strong style="color: var(--success);">✓ Implementation complete and achieving target performance.</strong>
                        </p>
                    </div>

                    <div class="pipeline-step" style="background: linear-gradient(135deg, rgba(255, 168, 1, 0.15), rgba(255, 147, 100, 0.15));">
                        <div class="step-number">2</div>
                        <h3 class="step-title">Router Classifier</h3>
                        <div class="step-model">View Angle Classification</div>
                        <div class="step-specs" style="background: rgba(255, 168, 1, 0.2); border-left: 3px solid var(--warning);">
                            <strong>Stage 2: Currently Training</strong><br>
                            Decision: Clear side view? Angled view? Turbid water?<br>
                            Output: Branch routing (3 paths)<br>
                            Method: Lightweight CNN classifier
                        </div>
                        <p style="color: var(--text-muted); margin-top: 1rem; line-height: 1.7;">
                            <strong style="color: var(--warning);">⟳ Active development:</strong> The router classifier model is in training phase. We are developing the CNN architecture that will intelligently route fish detections to specialized processing branches based on view angle and water clarity conditions.
                        </p>
                        <ul class="feature-list" style="margin-top: 1rem;">
                            <li><strong>Clear Side View Branch:</strong> Optimal visibility, proceeds to keypoint detection</li>
                            <li><strong>Angled View Branch:</strong> Partial occlusion, uses ellipse fitting</li>
                            <li><strong>Turbidity Branch:</strong> Poor visibility, employs bio-inspired features</li>
                        </ul>
                    </div>

                    <div class="pipeline-step">
                        <div class="step-number">3</div>
                        <h3 class="step-title">Branch-Specific Processing</h3>
                        <div class="step-model">Adaptive Length Estimation</div>
                        <div class="step-specs">
                            Clear View: YOLOv8-Pose keypoint detection (head, tail, dorsal)<br>
                            Angled View: Ellipse fitting algorithms<br>
                            Turbid Water: Bio-inspired feature extraction
                        </div>
                        <p style="color: var(--text-muted); margin-top: 1rem; line-height: 1.7;">
                            Each branch applies specialized algorithms optimized for specific viewing conditions. The clear view branch uses precise keypoint localization, while turbid water conditions rely on robust bio-inspired features that work despite poor visibility.
                        </p>
                    </div>

                    <div class="pipeline-step">
                        <div class="step-number">3.1</div>
                        <h3 class="step-title">Stereo Depth → 3D Length</h3>
                        <div class="step-model">3D Reconstruction & Measurement</div>
                        <div class="step-specs">
                            Input: Keypoints + stereo camera pair<br>
                            Method: Disparity calculation & triangulation<br>
                            Output: Real-world 3D coordinates & length (cm)
                        </div>
                        <ul class="feature-list">
                            <li>Stereo matching for depth estimation</li>
                            <li>3D coordinate triangulation</li>
                            <li>Sub-centimeter precision (&lt;2% error)</li>
                            <li>Camera calibration compensation</li>
                        </ul>
                    </div>

                    <div class="pipeline-step">
                        <div class="step-number">4</div>
                        <h3 class="step-title">Biomass MLP Regressor</h3>
                        <div class="step-model">Weight Estimation Neural Network</div>
                        <div class="step-specs">
                            Input: 3D length + volume features<br>
                            Architecture: Multi-layer perceptron (MLP)<br>
                            Output: Biomass estimate (kg) with &lt;5% error
                        </div>
                        <p style="color: var(--text-muted); margin-top: 1rem; line-height: 1.7;">
                            The MLP regressor converts 3D measurements into accurate biomass estimates using learned length-weight relationships specific to tuna species. This stage integrates all geometric features to produce final weight predictions.
                        </p>
                    </div>

                    <div class="pipeline-step">
                        <div class="step-number">5</div>
                        <h3 class="step-title">Behavior Recognition</h3>
                        <div class="step-model">Activity Classification & Monitoring</div>
                        <div class="step-specs">
                            Analysis: Feeding, hypoxia, escape behaviors<br>
                            Method: Temporal CNN or LSTM<br>
                            Output: Behavioral alerts & health indicators
                        </div>
                        <ul class="feature-list">
                            <li>Real-time feeding detection for optimal feeding schedules</li>
                            <li>Hypoxia stress detection for water quality alerts</li>
                            <li>Escape behavior recognition for net integrity monitoring</li>
                            <li>Temporal pattern analysis for health trends</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="grid-2" style="margin-top: 3rem;">
                <div class="glass-card">
                    <h3 style="font-size: 1.5rem; margin-bottom: 1.5rem; color: white;">Adaptive Routing Intelligence</h3>
                    <p style="color: var(--text-muted); line-height: 1.8;">
                        The router classifier (Stage 2) is crucial for system robustness. By analyzing each detected fish and determining the optimal processing path, the system maintains high accuracy across varying underwater conditions:
                    </p>
                    <ul class="feature-list" style="margin-top: 1rem;">
                        <li><strong>Clear conditions:</strong> Maximum precision with keypoint detection</li>
                        <li><strong>Partial occlusion:</strong> Geometric fallback with ellipse fitting</li>
                        <li><strong>Turbid water:</strong> Robust features that penetrate poor visibility</li>
                        <li><strong>Mixed scenarios:</strong> Per-fish adaptive processing</li>
                    </ul>
                </div>

                <div class="glass-card">
                    <h3 style="font-size: 1.5rem; margin-bottom:1.5rem; color: white;">Integration Benefits</h3>
                    <p style="color: var(--text-muted); line-height: 1.8;">
                        This multi-stage architecture provides comprehensive monitoring capabilities:
                    </p>
                    <ul class="feature-list" style="margin-top: 1rem;">
                        <li><strong>Accuracy:</strong> &lt;2% length error, &lt;5% biomass error</li>
                        <li><strong>Robustness:</strong> Functions in varied underwater conditions</li>
                        <li><strong>Real-time:</strong> 20-40 FPS on Raspberry Pi 4</li>
                        <li><strong>Actionable insights:</strong> Biomass + behavior analysis</li>
                        <li><strong>Scalability:</strong> Modular design for easy upgrades</li>
                    </ul>
                </div>
            </div>

            <div class="glass-card" style="margin-top: 3rem; background: linear-gradient(135deg, rgba(102, 126, 234, 0.08), rgba(240, 147, 251, 0.08));">
                <h3 style="font-size: 1.8rem; margin-bottom: 2rem; color: white;">Current Development Status</h3>
                <div style="padding: 1.5rem; background: rgba(102, 126, 234, 0.15); border-radius: 12px; border-left: 4px solid var(--primary); margin-bottom: 1rem;">
                    <p style="color: var(--text); font-weight: 600; margin-bottom: 1rem;">
                        Stage 1 (YOLOv8n Detection): Implementation complete and optimizing
                    </p>
                    <p style="color: var(--text-muted); line-height: 1.7;">
                        Stage 1 has achieved target performance, delivering 35-40 FPS with INT8 quantization and maintaining &lt;3% detection error. The foundation is solid and ready for integration.
                    </p>
                </div>
                <div style="padding: 1.5rem; background: rgba(255, 168, 1, 0.15); border-radius: 12px; border-left: 4px solid var(--warning);">
                    <p style="color: var(--text); font-weight: 600; margin-bottom: 1rem;">
                        Stage 2 (Router Classifier): Currently training and developing
                    </p>
                    <p style="color: var(--text-muted); line-height: 1.7;">
                        The router classifier model is in active training phase. We are developing the CNN architecture that will intelligently route fish detections to specialized processing branches based on view angle and water clarity conditions.
                    </p>
                </div>
                <div style="margin-top: 2rem; display: grid; grid-template-columns: repeat(5, 1fr); gap: 1rem;">
                    <div style="text-align: center; padding: 1rem; background: rgba(11, 232, 129, 0.2); border-radius: 12px; box-shadow: 0 4px 12px rgba(11, 232, 129, 0.3);">
                        <div style="font-size: 1.5rem; color: var(--success); margin-bottom: 0.5rem;">✓</div>
                        <div style="color: var(--text); font-size: 0.9rem; font-weight: 600;">Stage 1</div>
                        <div style="color: var(--success); font-size: 0.75rem; margin-top: 0.25rem;">Complete</div>
                    </div>
                    <div style="text-align: center; padding: 1rem; background: rgba(255, 168, 1, 0.2); border-radius: 12px; box-shadow: 0 4px 12px rgba(255, 168, 1, 0.3);">
                        <div style="font-size: 1.5rem; color: var(--warning); margin-bottom: 0.5rem;">⟳</div>
                        <div style="color: var(--text); font-size: 0.9rem; font-weight: 600;">Stage 2</div>
                        <div style="color: var(--warning); font-size: 0.75rem; margin-top: 0.25rem;">In Training</div>
                    </div>
                    <div style="text-align: center; padding: 1rem; background: rgba(156, 163, 175, 0.1); border-radius: 12px;">
                        <div style="font-size: 1.5rem; color: var(--text-muted); margin-bottom: 0.5rem;">○</div>
                        <div style="color: var(--text-muted); font-size: 0.9rem;">Stage 3</div>
                        <div style="color: var(--text-muted); font-size: 0.75rem; margin-top: 0.25rem;">Pending</div>
                    </div>
                    <div style="text-align: center; padding: 1rem; background: rgba(156, 163, 175, 0.1); border-radius: 12px;">
                        <div style="font-size: 1.5rem; color: var(--text-muted); margin-bottom: 0.5rem;">○</div>
                        <div style="color: var(--text-muted); font-size: 0.9rem;">Stage 4</div>
                        <div style="color: var(--text-muted); font-size: 0.75rem; margin-top: 0.25rem;">Pending</div>
                    </div>
                    <div style="text-align: center; padding: 1rem; background: rgba(156, 163, 175, 0.1); border-radius: 12px;">
                        <div style="font-size: 1.5rem; color: var(--text-muted); margin-bottom: 0.5rem;">○</div>
                        <div style="color: var(--text-muted); font-size: 0.9rem;">Stage 5</div>
                        <div style="color: var(--text-muted); font-size: 0.75rem; margin-top: 0.25rem;">Pending</div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Overview Section -->
        <section id="overview" class="fade-in">
            <div class="section-header">
                <h2 class="section-title">Algorithm Architecture</h2>
                <p class="section-subtitle">Three-stage detection, keypoint, and 3D reconstruction pipeline</p>
            </div>

            <div class="pipeline">
                <div class="pipeline-step">
                    <div class="step-number">1</div>
                    <h3 class="step-title">Fish Detection</h3>
                    <div class="step-model">YOLOv8-Nano</div>
                    <div class="step-specs">
                        Input: 640×640 pixels<br>
                        FPS: 20-30 on RPi 4<br>
                        Output: Bounding boxes
                    </div>
                    <ul class="feature-list">
                        <li>Real-time object detection</li>
                        <li>Optimized for edge devices</li>
                        <li>Enhanced anchor-free detection</li>
                        <li>Better small object detection</li>
                    </ul>
                </div>

                <div class="pipeline-step">
                    <div class="step-number">2</div>
                    <h3 class="step-title">Keypoint Detection</h3>
                    <div class="step-model">YOLOv8-Pose</div>
                    <div class="step-specs">
                        Input: Cropped fish regions<br>
                        Output: 8-17 keypoints<br>
                        Accuracy: High precision
                    </div>
                    <ul class="feature-list">
                        <li>Head & tail localization</li>
                        <li>Anatomical feature points</li>
                        <li>Pose estimation capability</li>
                        <li>Multi-scale detection</li>
                    </ul>
                </div>

                <div class="pipeline-step">
                    <div class="step-number">3</div>
                    <h3 class="step-title">3D Reconstruction</h3>
                    <div class="step-model">Stereo Depth CNN</div>
                    <div class="step-specs">
                        Method: Stereo matching<br>
                        Output: Depth maps<br>
                        Precision: Sub-centimeter
                    </div>
                    <ul class="feature-list">
                        <li>Disparity calculation</li>
                        <li>3D coordinate mapping</li>
                        <li>Volume estimation</li>
                        <li>Biomass calculation</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- YOLOv8 Section -->
        <section id="yolov8" class="fade-in">
            <div class="section-header">
                <h2 class="section-title">YOLOv8 Architecture</h2>
                <p class="section-subtitle">State-of-the-art object detection for fish monitoring</p>
            </div>

            <div class="glass-card">
                <h3 style="font-size: 1.8rem; margin-bottom: 2rem; color: white;">Why YOLOv8?</h3>
                <div class="grid-2">
                    <div>
                        <h4 style="color: var(--secondary); margin-bottom: 1rem; font-size: 1.3rem;">Key Advantages</h4>
                        <ul class="feature-list">
                            <li>Anchor-free detection - improved accuracy for varying fish sizes</li>
                            <li>Enhanced backbone with C2f modules for better feature extraction</li>
                            <li>Optimized for edge devices with quantization support</li>
                            <li>Built-in pose estimation (YOLOv8-Pose) for keypoint detection</li>
                            <li>Superior small object detection crucial for juvenile fish</li>
                            <li>Faster inference speed compared to previous YOLO versions</li>
                        </ul>
                    </div>
                    <div>
                        <h4 style="color: var(--secondary); margin-bottom: 1rem; font-size: 1.3rem;">Model Variants</h4>
                        <ul class="feature-list">
                            <li><strong>YOLOv8n (Nano):</strong> Fastest, best for Raspberry Pi</li>
                            <li><strong>YOLOv8s (Small):</strong> Balanced speed and accuracy</li>
                            <li><strong>YOLOv8m (Medium):</strong> Better accuracy, requires more power</li>
                            <li><strong>YOLOv8-Pose:</strong> Specialized for keypoint detection</li>
                            <li><strong>INT8 Quantization:</strong> 4x faster inference on edge</li>
                            <li><strong>TensorRT Optimization:</strong> GPU acceleration support</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="chart-box" style="margin-top: 3rem;">
                <h3 class="chart-title">YOLOv8 Performance Metrics</h3>
                <div class="chart-wrapper">
                    <canvas id="yolov8Chart"></canvas>
                </div>
            </div>
        </section>

        <!-- Comparison Section -->
        <section id="comparison" class="fade-in">
            <div class="section-header">
                <h2 class="section-title">Model Comparison</h2>
                <p class="section-subtitle">YOLOv8-Nano vs MobileNetV3 for fish detection</p>
            </div>

            <div class="comparison-table">
                <table>
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>YOLOv8-Nano</th>
                            <th>MobileNetV3 + SSD</th>
                            <th>Winner</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Parameters</strong></td>
                            <td>3.2M</td>
                            <td>5.4M</td>
                            <td><span class="badge badge-best">YOLOv8</span></td>
                        </tr>
                        <tr>
                            <td><strong>Model Size</strong></td>
                            <td>6.4 MB</td>
                            <td>22 MB</td>
                            <td><span class="badge badge-best">YOLOv8</span></td>
                        </tr>
                        <tr>
                            <td><strong>FPS on RPi 4 (FP32)</strong></td>
                            <td>20-25 FPS</td>
                            <td>15-18 FPS</td>
                            <td><span class="badge badge-best">YOLOv8</span></td>
                        </tr>
                        <tr>
                            <td><strong>FPS on RPi 4 (INT8)</strong></td>
                            <td>35-40 FPS</td>
                            <td>25-30 FPS</td>
                            <td><span class="badge badge-best">YOLOv8</span></td>
                        </tr>
                        <tr>
                            <td><strong>mAP@0.5 (COCO)</strong></td>
                            <td>37.3%</td>
                            <td>24.8%</td>
                            <td><span class="badge badge-best">YOLOv8</span></td>
                        </tr>
                        <tr>
                            <td><strong>Small Object Detection</strong></td>
                            <td>Excellent</td>
                            <td>Moderate</td>
                            <td><span class="badge badge-best">YOLOv8</span></td>
                        </tr>
                        <tr>
                            <td><strong>Training Complexity</strong></td>
                            <td>Simple (Ultralytics)</td>
                            <td>Complex (TensorFlow)</td>
                            <td><span class="badge badge-best">YOLOv8</span></td>
                        </tr>
                        <tr>
                            <td><strong>Deployment Ease</strong></td>
                            <td>Very Easy (ONNX, TFLite)</td>
                            <td>Easy (TFLite)</td>
                            <td><span class="badge badge-best">YOLOv8</span></td>
                        </tr>
                        <tr>
                            <td><strong>Pose Estimation Support</strong></td>
                            <td>Native (YOLOv8-Pose)</td>
                            <td>Requires additional model</td>
                            <td><span class="badge badge-best">YOLOv8</span></td>
                        </tr>
                        <tr>
                            <td><strong>Memory Usage (Inference)</strong></td>
                            <td>~200 MB</td>
                            <td>~150 MB</td>
                            <td><span class="badge badge-recommended">MobileNet</span></td>
                        </tr>
                        <tr>
                            <td><strong>Power Consumption</strong></td>
                            <td>Moderate</td>
                            <td>Lower</td>
                            <td><span class="badge badge-recommended">MobileNet</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <div class="chart-box" style="margin-top: 3rem;">
                <h3 class="chart-title">Performance Comparison: YOLOv8 vs MobileNetV3</h3>
                <div class="chart-wrapper">
                    <canvas id="comparisonChart"></canvas>
                </div>
            </div>

            <div class="glass-card" style="margin-top: 3rem;">
                <h3 style="font-size: 1.8rem; margin-bottom: 2rem; color: white;">Recommendation: YOLOv8-Nano</h3>
                <p style="color: var(--text-muted); font-size: 1.1rem; line-height: 1.8;">
                    <strong style="color: var(--primary);">YOLOv8-Nano is the recommended choice</strong> for the MARINA Plus project due to its superior accuracy (37.3% mAP vs 24.8%), better small object detection crucial for juvenile tuna, native pose estimation support, and faster inference speed on Raspberry Pi 4. While MobileNetV3 has slightly lower memory usage and power consumption, YOLOv8's overall performance advantages, ease of deployment, and comprehensive ecosystem make it the optimal solution for real-time fish detection and biomass estimation.
                </p>
                <div style="margin-top: 2rem; padding: 1.5rem; background: rgba(102, 126, 234, 0.1); border-radius: 12px; border-left: 4px solid var(--primary);">
                    <p style="color: var(--text); font-weight: 600;">Key Advantage: YOLOv8-Nano achieves 20-25 FPS on Raspberry Pi 4 with FP32 precision and 35-40 FPS with INT8 quantization, exceeding the 15 FPS minimum requirement while maintaining superior detection accuracy.</p>
                </div>
            </div>
        </section>

        <!-- Hardware Section -->
        <section id="hardware" class="fade-in">
            <div class="section-header">
                <h2 class="section-title">Hardware Analysis</h2>
                <p class="section-subtitle">Raspberry Pi 4 and recommended alternatives</p>
            </div>

            <div class="grid-3">
                <div class="hardware-card">
                    <span class="hw-badge badge-budget">Our Choice</span>
                    <h3 class="hw-title">Raspberry Pi 4 (8GB)</h3>
                    
                    <div class="performance-box">
                        <div class="perf-value">20-25 FPS</div>
                        <div class="perf-label">YOLOv8-Nano Performance</div>
                    </div>

                    <div class="spec-section">
                        <h4 class="spec-title">Specifications</h4>
                        <ul class="spec-list pros">
                            <li>Cost: ~$75 (very affordable)</li>
                            <li>CPU: Quad-core ARM Cortex-A72 @ 1.5GHz</li>
                            <li>RAM: 8GB LPDDR4</li>
                            <li>GPU: VideoCore VI (OpenGL ES 3.0)</li>
                            <li>Power: 15W typical consumption</li>
                            <li>I/O: 2× USB 3.0, 2× USB 2.0, Ethernet</li>
                            <li>Camera: 2× CSI ports for stereo setup</li>
                        </ul>
                    </div>

                    <div class="spec-section">
                        <h4 class="spec-title">Advantages</h4>
                        <ul class="spec-list pros">
                            <li>Extremely cost-effective solution</li>
                            <li>Large community and extensive documentation</li>
                            <li>Easy to deploy and maintain</li>
                            <li>Low power consumption for offshore use</li>
                            <li>Native camera support for stereo vision</li>
                            <li>Suitable for proof-of-concept and testing</li>
                        </ul>
                    </div>

                    <div class="spec-section">
                        <h4 class="spec-title">Limitations</h4>
                        <ul class="spec-list cons">
                            <li>Limited to 20-25 FPS (meets minimum 15 FPS)</li>
                            <li>No dedicated AI accelerator</li>
                            <li>Thermal throttling under sustained load</li>
                            <li>CPU-based inference only (no GPU acceleration)</li>
                            <li>May struggle with full 3-stage pipeline</li>
                            <li>Limited scalability for multi-camera setups</li>
                        </ul>
                    </div>

                    <div class="spec-section">
                        <h4 class="spec-title">Optimization for RPi 4</h4>
                        <ul class="spec-list pros">
                            <li>Use TensorFlow Lite with XNNPACK delegate</li>
                            <li>Apply INT8 quantization (4x speedup)</li>
                            <li>Reduce input resolution to 416×416 if needed</li>
                            <li>Enable multi-threading (4 cores)</li>
                            <li>Use hardware H.264 encoding for streaming</li>
                            <li>Add active cooling (heatsink + fan)</li>
                        </ul>
                    </div>
                </div>

                <div class="hardware-card">
                    <span class="hw-badge badge-recommended">Recommended</span>
                    <h3 class="hw-title">NVIDIA Jetson Nano</h3>
                    
                    <div class="performance-box">
                        <div class="perf-value">45-60 FPS</div>
                        <div class="perf-label">YOLOv8-Nano Performance</div>
                    </div>

                    <div class="spec-section">
                        <h4 class="spec-title">Specifications</h4>
                        <ul class="spec-list pros">
                            <li>Cost: ~$150-200 (affordable)</li>
                            <li>CPU: Quad-core ARM Cortex-A57 @ 1.43GHz</li>
                            <li>GPU: 128-core Maxwell (472 GFLOPS)</li>
                            <li>RAM: 4GB LPDDR4</li>
                            <li>AI Performance: 0.5 TOPS</li>
                            <li>Power: 10W (5W mode available)</li>
                            <li>Camera: 2× CSI-2 MIPI ports</li>
                        </ul>
                    </div>

                    <div class="spec-section">
                        <h4 class="spec-title">Advantages</h4>
                        <ul class="spec-list pros">
                            <li>3x faster than Raspberry Pi 4</li>
                            <li>Native CUDA support for GPU acceleration</li>
                            <li>TensorRT optimization (2-3x speedup)</li>
                            <li>Better thermal management</li>
                            <li>Handles full 3-stage pipeline efficiently</li>
                            <li>Suitable for production deployment</li>
                            <li>Multiple camera input support</li>
                        </ul>
                    </div>

                    <div class="spec-section">
                        <h4 class="spec-title">Limitations</h4>
                        <ul class="spec-list cons">
                            <li>Higher cost than Raspberry Pi</li>
                            <li>Only 4GB RAM (vs 8GB on RPi 4)</li>
                            <li>Older GPU architecture</li>
                            <li>Requires more setup expertise</li>
                        </ul>
                    </div>
                </div>

                <div class="hardware-card">
                    <span class="hw-badge badge-best">Best Performance</span>
                    <h3 class="hw-title">NVIDIA Jetson Xavier NX</h3>
                    
                    <div class="performance-box">
                        <div class="perf-value">100+ FPS</div>
                        <div class="perf-label">YOLOv8-Nano Performance</div>
                    </div>

                    <div class="spec-section">
                        <h4 class="spec-title">Specifications</h4>
                        <ul class="spec-list pros">
                            <li>Cost: ~$400-500 (premium)</li>
                            <li>CPU: 6-core NVIDIA Carmel ARM @ 1.9GHz</li>
                            <li>GPU: 384-core Volta (21 TOPS)</li>
                            <li>RAM: 8GB LPDDR4x</li>
                            <li>AI Performance: 21 TOPS</li>
                            <li>Power: 10-15W efficient modes</li>
                            <li>Camera: Up to 6 CSI-2 cameras</li>
                        </ul>
                    </div>

                    <div class="spec-section">
                        <h4 class="spec-title">Advantages</h4>
                        <ul class="spec-list pros">
                            <li>5x faster than Jetson Nano</li>
                            <li>Superior AI performance (21 TOPS)</li>
                            <li>Handles multiple camera streams</li>
                            <li>Real-time 3-stage pipeline processing</li>
                            <li>Industrial temperature range</li>
                            <li>Best for production and scaling</li>
                            <li>Deep Learning Accelerator (DLA) support</li>
                        </ul>
                    </div>

                    <div class="spec-section">
                        <h4 class="spec-title">Limitations</h4>
                        <ul class="spec-list cons">
                            <li>Higher cost (~$400-500)</li>
                            <li>Overkill for single-camera setup</li>
                            <li>Complex deployment setup</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="chart-box" style="margin-top: 3rem;">
                <h3 class="chart-title">Hardware Performance Comparison</h3>
                <div class="chart-wrapper">
                    <canvas id="hardwareChart"></canvas>
                </div>
            </div>

            <div class="glass-card" style="margin-top: 3rem;">
                <h3 style="font-size: 1.8rem; margin-bottom: 2rem; color: white;">Hardware Recommendation</h3>
                <div class="grid-2">
                    <div>
                        <h4 style="color: var(--secondary); margin-bottom: 1rem; font-size: 1.3rem;">For Your Use Case (RPi 4)</h4>
                        <p style="color: var(--text-muted); line-height: 1.8;">
                            <strong style="color: var(--primary);">Raspberry Pi 4 (8GB) is adequate</strong> for initial deployment and proof-of-concept. With YOLOv8-Nano optimized using INT8 quantization and TFLite, you can achieve 35-40 FPS, which exceeds the 15 FPS minimum requirement. This is cost-effective for testing the algorithm and validating the approach.
                        </p>
                        <ul class="feature-list" style="margin-top: 1rem;">
                            <li>Achieves 20-25 FPS (FP32) or 35-40 FPS (INT8)</li>
                            <li>Meets minimum performance requirements</li>
                            <li>Very cost-effective at ~$75</li>
                            <li>Ideal for proof-of-concept phase</li>
                        </ul>
                    </div>
                    <div>
                        <h4 style="color: var(--secondary); margin-bottom: 1rem; font-size: 1.3rem;">For Production Deployment</h4>
                        <p style="color: var(--text-muted); line-height: 1.8;">
                            <strong style="color: var(--primary);">Upgrade to Jetson Nano or Xavier NX</strong> for production deployment. The Jetson Nano offers 3x better performance at a reasonable price point (~$150-200), while Xavier NX provides enterprise-grade performance for multi-camera setups and complex processing pipelines.
                        </p>
                        <ul class="feature-list" style="margin-top: 1rem;">
                            <li>Jetson Nano: 45-60 FPS, best value for production</li>
                            <li>Xavier NX: 100+ FPS, handles full pipeline</li>
                            <li>GPU acceleration with TensorRT</li>
                            <li>Better thermal management for 24/7 operation</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>

        <!-- Implementation Details -->
        <section id="implementation" class="fade-in">
            <div class="section-header">
                <h2 class="section-title">Implementation Details</h2>
                <p class="section-subtitle">Technical specifications and optimization strategies</p>
            </div>

            <div class="grid-2">
                <div class="glass-card">
                    <h3 style="font-size: 1.5rem; margin-bottom: 1.5rem; color: white;">YOLOv8 Configuration</h3>
                    <pre style="background: rgba(0,0,0,0.3); padding: 1.5rem; border-radius: 12px; overflow-x: auto; color: var(--text); line-height: 1.8;">
<span style="color: var(--secondary);"># Model Selection</span>
model: yolov8n.pt  <span style="color: var(--text-muted);"># Nano variant</span>
input_size: 640x640

<span style="color: var(--secondary);"># Training Configuration</span>
batch_size: 16
epochs: 100
learning_rate: 0.01
optimizer: SGD
augmentation:
  - rotation: ±15°
  - brightness: ±20%
  - horizontal_flip: true
  
<span style="color: var(--secondary);"># Optimization</span>
quantization: INT8
export_format: TFLite
optimization: XNNPACK</pre>
                </div>

                <div class="glass-card">
                    <h3 style="font-size: 1.5rem; margin-bottom: 1.5rem; color: white;">Raspberry Pi Optimization</h3>
                    <pre style="background: rgba(0,0,0,0.3); padding: 1.5rem; border-radius: 12px; overflow-x: auto; color: var(--text); line-height: 1.8;">
<span style="color: var(--secondary);"># System Configuration</span>
CPU_cores: 4 (all utilized)
Thread_count: 4
GPU_delegate: OpenGL ES 3.0

<span style="color: var(--secondary);"># Model Optimization</span>
precision: INT8
delegate: XNNPACK
input_resolution: 640x640
batch_size: 1

<span style="color: var(--secondary);"># Performance Tuning</span>
enable_threading: true
enable_cpu_affinity: true
thermal_management: active_cooling</pre>
                </div>
            </div>

            <div class="chart-box" style="margin-top: 3rem;">
                <h3 class="chart-title">Optimization Impact on Performance</h3>
                <div class="chart-wrapper">
<canvas id="optimizationChart"></canvas>
</div>
</div><div class="glass-card" style="margin-top: 3rem;">
            <h3 style="font-size: 1.8rem; margin-bottom: 2rem; color: white;">Expected Performance Metrics</h3>
            <div class="grid-3" style="margin-top: 2rem;">
                <div style="text-align: center; padding: 2rem; background: rgba(102, 126, 234, 0.1); border-radius: 16px;">
                    <div style="font-size: 2.5rem; font-weight: 900; color: var(--primary); margin-bottom: 0.5rem;">35-40</div>
                    <div style="color: var(--text-muted);">FPS on RPi 4 (INT8)</div>
                </div>
                <div style="text-align: center; padding: 2rem; background: rgba(102, 126, 234, 0.1); border-radius: 16px;">
                    <div style="font-size: 2.5rem; font-weight: 900; color: var(--primary); margin-bottom: 0.5rem;">&lt;2%</div>
                    <div style="color: var(--text-muted);">Length Measurement Error</div>
                </div>
                <div style="text-align: center; padding: 2rem; background: rgba(102, 126, 234, 0.1); border-radius: 16px;">
                    <div style="font-size: 2.5rem; font-weight: 900; color: var(--primary); margin-bottom: 0.5rem;">&lt;5%</div>
                    <div style="color: var(--text-muted);">Biomass Estimation Error</div>
                </div>
            </div>
        </div>
    </section>
</div>

<footer>
    <div class="footer-content">
        <p class="footer-text">MARINA Plus Technical Algorithm Report - YOLOv8 Implementation for Tuna Fish Detection</p>
        <p class="footer-text" style="margin-top: 1rem;">&copy; 2025 WES Trade Ltd. All rights reserved.</p>
    </div>
</footer>

<script>
    const chartColors = {
        primary: '#667eea',
        secondary: '#f093fb',
        accent: '#4facfe',
        success: '#0be881',
        warning: '#ffa801',
        danger: '#ff3838'
    };

    const chartDefaults = {
        responsive: true,
        maintainAspectRatio: false,
        plugins: {
            legend: {
                labels: {
                    color: '#e4e7eb',
                    font: { size: 14, weight: '600' }
                }
            }
        },
        scales: {
            y: {
                ticks: { color: '#9ca3af' },
                grid: { color: 'rgba(255, 255, 255, 0.05)' }
            },
            x: {
                ticks: { color: '#9ca3af' },
                grid: { color: 'rgba(255, 255, 255, 0.05)' }
            }
        }
    };

    new Chart(document.getElementById('yolov8Chart'), {
        type: 'bar',
        data: {
            labels: ['YOLOv8n', 'YOLOv8s', 'YOLOv8m', 'YOLOv8l', 'YOLOv8x'],
            datasets: [{
                label: 'mAP@0.5',
                data: [37.3, 44.9, 50.2, 52.9, 53.9],
                backgroundColor: chartColors.primary,
                borderRadius: 8
            }, {
                label: 'FPS on RPi 4',
                data: [25, 15, 8, 4, 2],
                backgroundColor: chartColors.secondary,
                borderRadius: 8
            }]
        },
        options: chartDefaults
    });

    new Chart(document.getElementById('comparisonChart'), {
        type: 'radar',
        data: {
            labels: ['Accuracy', 'Speed', 'Model Size', 'Memory', 'Ease of Use', 'Features'],
            datasets: [{
                label: 'YOLOv8-Nano',
                data: [85, 90, 95, 70, 95, 100],
                backgroundColor: 'rgba(102, 126, 234, 0.2)',
                borderColor: chartColors.primary,
                borderWidth: 3,
                pointBackgroundColor: chartColors.primary
            }, {
                label: 'MobileNetV3',
                data: [65, 70, 80, 85, 75, 60],
                backgroundColor: 'rgba(240, 147, 251, 0.2)',
                borderColor: chartColors.secondary,
                borderWidth: 3,
                pointBackgroundColor: chartColors.secondary
            }]
        },
        options: {
            ...chartDefaults,
            scales: {
                r: {
                    ticks: { color: '#9ca3af', backdropColor: 'transparent' },
                    grid: { color: 'rgba(255, 255, 255, 0.1)' },
                    pointLabels: { color: '#e4e7eb' }
                }
            }
        }
    });

    new Chart(document.getElementById('hardwareChart'), {
        type: 'bar',
        data: {
            labels: ['Raspberry Pi 4', 'Jetson Nano', 'Jetson Xavier NX'],
            datasets: [{
                label: 'FPS (YOLOv8-Nano)',
                data: [25, 50, 100],
                backgroundColor: chartColors.primary,
                borderRadius: 8
            }, {
                label: 'Cost ($)',
                data: [75, 175, 450],
                backgroundColor: chartColors.warning,
                borderRadius: 8
            }, {
                label: 'Power (W)',
                data: [15, 10, 15],
                backgroundColor: chartColors.success,
                borderRadius: 8
            }]
        },
        options: chartDefaults
    });

    new Chart(document.getElementById('optimizationChart'), {
        type: 'line',
        data: {
            labels: ['Baseline FP32', 'Multi-threading', '+ Input Resize', '+ INT8 Quantization', '+ XNNPACK'],
            datasets: [{
                label: 'FPS on Raspberry Pi 4',
                data: [12, 18, 22, 32, 38],
                backgroundColor: 'rgba(102, 126, 234, 0.2)',
                borderColor: chartColors.primary,
                borderWidth: 3,
                fill: true,
                tension: 0.4,
                pointRadius: 6,
                pointBackgroundColor: chartColors.primary
            }]
        },
        options: chartDefaults
    });

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener('click', function (e) {
            e.preventDefault();
            const target = document.querySelector(this.getAttribute('href'));
            if (target) {
                target.scrollIntoView({ behavior: 'smooth', block: 'start' });
                
                document.querySelectorAll('nav a').forEach(link => {
                    link.classList.remove('active');
                });
                this.classList.add('active');
            }
        });
    });

    const observer = new IntersectionObserver((entries) => {
        entries.forEach(entry => {
            if (entry.isIntersecting) {
                entry.target.style.opacity = '1';
                entry.target.style.transform = 'translateY(0)';
            }
        });
    }, { threshold: 0.1 });

    document.querySelectorAll('.fade-in').forEach(el => {
        el.style.opacity = '0';
        el.style.transform = 'translateY(40px)';
        el.style.transition = 'all 0.8s ease-out';
        observer.observe(el);
    });
</script></body>
</html>
